Библиотека является результатом домашней работы для урока #3 курса "OTUS web-python".
Скрипт позволяет анализировать слова в файлах с расширением *.py. Файлы скачиваются из указанного склонированного репозитория GitHUB. Результаты анализа выводятся в консоль, либо в файл формата .json, .csv.


###Установка
Для работы требуется библиотеки pygit2 и nltk.

Перед первым вызовом скриптов установите пакет nltk:

     import nltk
     nltk.download('averaged_perceptron_tagged')

     
###Входные параметры
`-f, --filter` - поиск определенных смысловых единиц кода. Допустимые аргументы:
* `Function` - будут выведены только названия функций;
* `Local` - будут выведены только локальные переменные;
* `Name` - будут выведены все слова, используется по умолчанию.

`-o, --output` - задание формата вывода отчета. Допустимые аргументы:
* `console` - вывод результата в консоль, используется по умолчанию;
* `json` - вывод результата в файл `report.json` в директории скрипта;
* `csv` - вывод результата в файл `report.csv` в директории скрипта.

`-w, --wordtype` - задание фильтра слов по части речи, по умолчанию не применяется. Допустимые аргументы:
* `VB` - выбрать среди найденных слов глаголы;
* `NN` - выбрать среди найденных слов существительные.
     
###Описание работы

Скрипт анализирует входные параметры, переданные через консоль, и на их основе инициализирует менеджеры, отвечающие за работу с репозиториями, за анализ файлов и за вывод отчета.

    _outputs_dict = {'console': ConsoleOutputReport(),
                     'json': JsonOutputReport(),
                     'csv': CsvOutputReport()}

    _filters_dict = {'Function': AnalyzerFunctionNames(),
                     'Name': AnalyzerNames(),
                     'Local': AnalyzerLocalVariables()}

    args = parser.parse_args()
    
    filter = args.filter if args.filter else "Name"
    wordtype = args.wordtype
    output = args.output if args.output else "console"

    output_manager = _outputs_dict.get(output, ConsoleOutputReport)
    analyzer_manager = _filters_dict.get(filter, AnalyzerNames)
    repository_manager = GithubRepositoryClone()
    
Клонируем репозиторий GitHUB в указанную папку:

    repository_manager.clone_repository_by_url(repository_url, path_to)
    
Указываем папку с клонированным репозиторием

    analyzer_manager.set_path(path_to)
    
Далее, через последовательные вызовы методов `analyzer_manager` конфигурируем, в какой форме отчет мы хотим получить:

* `all()` - выводить все найденные результаты; используется по умолчанию;
* `top(top_size=10)` - выводить первые `top_size` результатов, после сортировки по частоте использования;
* `split()` - разбить в отчете слова в змеином регистре на отдельные;
* `filter_verb()` - отфильтровать результат, оставив только глаголы;
* `filter_noun()` - отфильтровать результат, оставив только существительные.

Методы будут применены к списку слов в порядке вызова:

    analyzer_manager.split().filter_verb().top(5)
В примере выше слова результата сначала будут переведены со змеиного регистра, с разбивкой на отдельные, потом из них будут получены глаголы, из которых взято 5 самых часто встречающихся.

Получаем настроенный отчет:

    report = analyzer_manager.report.create_report()
Передаем его менеджеру вывода:

    output_manager.output_report(report)
    
###Предусмотренные возможности:
1. Для получения кода не только с Гитхаба пишем свой класс менеджера, унаследованный от интерфейса `_AbstractRepositoryCloneClass`.

2. Для парсеров других ЯП - пишем свои классы менеджеров, унаследованные от абстрактного класса `_AbstractAnalyzerBuilder`. Метод `set_exp()` задает список расширений файлов, используемых в парсинге.

3. Для сохранения в других форматах - для каждого пишем класс, унаследованный от интерфейса `_AbstractOutputReportClass`.

4. Более сложные типы отчетов (в данной реализации - отдельных функций для формирования требований) - пишутся методы в классе `Report`, которые работают с текущим списком, добавляются методы доступа из `_AbstractAnalyzerBuilder` или производных классов.